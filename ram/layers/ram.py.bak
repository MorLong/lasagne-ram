# l_0 = init random in [-1, 1] (location x,y are in [-1, 1] for all application)
# h_0 = init zeros

# Step1: sensor
# x_t = rho(loc_tm1, x, patch, k)
#     = x[loc_tm1.y-patch/2:loc_tm1.y+patch/2, loc_tm1.x-patch/2:loc_tm1+patch/2]
#       x[loc_tm1.y-patch/2*2:loc_tm1.y+patch/2*2, loc_tm1.x-patch/2*2:loc_tm1+patch/2*2]
#       x[loc_tm1.y-patch/2*4 ....]
#       x[loc_tm1.y-patch/2*(2^(k-1)):loc_tm1.y+patch/2*(2^(k-1)), loc_tm1.x-patch/2*(2^(k-1)):loc_tm1.x+patch/2*(2^(k-1))]

# Step2: glimps network
# g_t = f_g(x_t, loc_tm1)                          (256 units)
#     = relu(W_1 * h_g + W_2 * h_l + (b_1+b_2)) 
#           where h_g = relu(W_3 * x_t + b_3)    (128 units)
#                 h_l = relu(W_4 * loc_tm1 + b_4)  (128 units)

# Step3: core network
# h_t = f_h(h_tm1, g_t)                          (256 units)
#     = W_5 * h_tm1 + W_6 * g_t + (b_5 + b_6)    (for classification)

# Step4: actions
# Step4a: location network                       (2 units)
# l_t ~ P(l_t | f_l(h_t)) = N(l_t | f_l(h_t), [[sigma^2, 0], [0, sigma^2]]) (= Gaussian with fixed variance)
#     where f_l(h_t) = W_7 * h_t + b_7
# Step4b: env action network                     (10 units for MNIST)
# a_t ~ P(a_t | f_a(h_t)) = Bernoulli(f_a(h_t))
#     where f_a(h_t) = softmax(W_8 * h_t + b_8)
# => a_T ~ P(a_T | f_a(h_T)) = Bernoulli(f_a(h_T))
#     where f_a(h_T) = softmax(W_8 * h_t + b_8)

# Step5: loss and grad
# Step5a: reinforcement learning loss and its grad
# loss1 = 1 / M * sum_i_{1..M}{r_T^i}  where r_T is 1 (if correct) or 0 (if incorrect) 
# grad1 = 1 / M * sum_i_{1..M}{theano.grad( sum_t_{0..T}{logN(l_t | f_l(h_t))} ) * (R^i - b) )}
#           where R^i = r_T^i = 1 (if correct) or 0 (if incorrect)
#                 b = mean(R^i)  (the value function???)
#                 b = sum_i_{1..M}{( theano.grad( sum_t_{0..T}{logN(l_t | f_l(h_t))} ) ** 2 ) * R^i } / sum_i_{1..M}{ theano.grad( sum_t_{0..T}{logN(l_t | f_l(h_t))} ) ** 2 }   
#                     (optimal baseline) 
# Step5b: supervised loss and its grad
# loss2 = 1 / M * sum_i_{1..M} cross_entroy_loss(groundtruth, a_T)
# grad2 = theano.grad(loss2)
# 
# grad1 is for location network W_7 and b_7
# grad2 is for the others, W_1, b_1, ..., W_8, b_8 except W_7 and b_7
#

import theano
import theano.tensor as T
import numpy as np
import lasagne
import cv2

img = cv2.imread('mnist.png')
(height, width, channels) = img.shape
print height, width, channels

k = 2 # number of glimps steps
patch = 8 # size of glimps patch
n_batch = 64 # number of batch 
n_steps = 4 # number of glimps steps
lambda_ = 10.0 # mixing ratio between 
n_classes = 10 

n_h_g = 128
n_h_l = 128
n_f_g = 256
n_f_h = 256
#n_f_l = 2

def __init__():
    # for glimps network, f_g  
    self.W_h_g = [] 
    for i in xrange(k): 
        self.W_h_g.append(self.add_param(init.GlorotNormal(), (channels*((patch*(2**i))**2), n_h_g), name='W_h_g'))
    self.b_h_g = self.add_param(init.Constant(0.), (n_h_g,), name='b_h_g')

    self.W_h_l = self.add_param(init.GlorotNormal(), (2, n_h_l), name='W_h_l')
    self.b_h_l = self.add_param(init.Constant(0.), (n_h_l,), name='b_h_l')

    self.W_f_g_1 = self.add_param(init.GlorotNormal(), (n_h_g, n_f_g), name='W_f_g_1')
    self.W_f_g_2 = self.add_param(init.GlorotNormal(), (n_h_l, n_f_g), name='W_f_g_2')
    self.b_f_g = self.add_param(init.Constant(0.), (n_f_g,), name='b_f_g')
    
    # for core network, f_h
    self.W_f_h_1 = self.add_param(init.GlorotNormal(), (n_f_g, n_f_h), name='W_f_h_1') 
    self.W_f_h_2 = self.add_param(init.GlorotNormal(), (n_f_g, n_f_h), name='W_f_h_2')
    self.b_f_h = self.add_param(init.Constant(0.), (n_f_h,), name='b_f_h') 
    
    # for action network (location) f_l
    self.W_f_l = self.add_param(init.GlorotNormal(), (n_f_h, 2), name='W_f_l')
    self.b_f_l = self.add_param(init.GlorotNormal(), (2,), name='b_f_')

    # for action network (classification) f_a
    self.W_classifier = self.add_param(init.GlorotNormal(), (n_f_h, n_classes), name='W_classifier')
    self.b_classifier = self.add_param(init.Constant(0.), (n_classes,), name='b_classifier')
 
    # for step 
    self._srng = RandomStreams(get_rng().randint(1, 2147462579))
    self.sigma = self.add_param(init.Constant(0.), (2, 2), regularizable=False, trainable=False, name='sigma')
    self.sigma.set_value(np.array([[0.01, 0.], [0., 0.01]]))

    
# get_ouput_for ---------------------------------------------------------------------
def rho(loc_tm1, x, height, width, patch=8, k=1):
    """
    return: 
        x_t = sensor output, 
              where x_t[i] = (n_batch x channels x patch*(2**i) x patch*(2**i)) for i in 0, ..., k 
            [python list, consisting of theano tensor variables]
         
    inputs: 
        loc_tm1 = location estimated at t-1
                = l(t-1) = y(t-1), x(t-1) 
                = (n_batch x 2) 
            [theano tensor variable] and recurrent
        x = original image 
          = (n_batch x channels x height x width)
            [theano tensor variable]
        height = image height = const
            [python integer]
        width = image width = const
            [python integer]
        patch = glimpse patch size = const
            [python integer]
        k = the number of scale = const
            [python integer]
    """
    x_t = []
    for i in xrange(k): 
        x_t_i = []
        for b in xrange(n_batch):
            range_template = theano.tensor.arange(0, patch*(2**i))

            #x_t[i] = x[loc_tm1[0][0]-patch/2*(2**i):loc_tm1[0][0]+patch/2*(2**i)]\
            #          [:, loc_tm1[0][1]-patch/2*(2**i):loc_tm1[0][1]+patch/2*(2**i)]
            #print loc_tm1[0]-patch/2*(2**i), loc_tm1[0]+patch/2*(2**i)
            y_start = theano.tensor.cast((1+loc_tm1[0][0])*height/2-patch/2*(2**i), 'int64')
            y_range = y_start + range_template
            x_start = theano.tensor.cast((1+loc_tm1[0][1])*width/2-patch/2*(2**i), 'int64')
            x_range = x_start + range_template
            tmp = x[:,:,y_range][:,:,:,x_range]
            x_t_i.append(tmp)
            #print "i: ", i, "  b: ", b
            #xx = x_range
            #yy = y_range
        x_t.append(T.concatenate(x_t_i, axis=0)) 
    return x_t #, xx, yy, val

def f_g(x_t, loc_tm1): 
    """
    g_t = f_g(x_t, loc_tm1)                          (256 units)
        = relu(W_1 * h_g + W_2 * h_l + (b_1+b_2))
          where h_g = relu(W_3 * x_t + b_3)    (128 units)
                h_l = relu(W_4 * loc_tm1 + b_4)  (128 units)

    return: 
      g_t = glimps output 
          = (n_batch x num hiddens of g_t)
          [theano tensor variable]

    inputs: 
      x_t = sensor output, 
          where x_t[i] = (n_batch x channels x patch*(2**i) x patch*(2**i) for i in 0, ..., k
          [python list, consisting of theano tensor variables]
      loc_tm1 = location estimated at t-1
              = l(t-1) = y(t-1), x(t-1) 
              = (n_batch x 2)
          [theano tensor variable] and recurrent

    parameters: 
      self.W_h_g = (k x num_inputs x num hiddens of h_g) 
      self.b_h_g = (num hiddens of h_g,)
        
      self.W_h_l = (2 x num hiddens of h_l)
      self.b_h_l = (num_hiddens of h_l,)

      self.W_f_g_1 = (num hiddens of h_g x num hiddens of g_t)
      self.W_f_g_2 = (num hiddens of h_l x num hiddens of g_t)
      self.b_f_g = (num hiddens of g_t,)

    """
    h_g = T.dot(x_t[0], self.W_h_g[0])
    for i in xrange(1, k): 
        h_g = h_g + T.dot(x_t[i], self.W_h_g[i])
    h_g = h_g + self.b_h_g.dimshuffle('x', 0) 
    h_g = lasagne.nonlinearities.rectify((h_g))

    h_l = lasagne.nonlinearities.rectify( \
              T.dot(loc_tm1, self.W_h_l) + 
              self.b_h_l.dimshuffle('x', 0))

    g_t = lasagne.nonlinearities.rectify( \ 
              T.dot(h_g, self.W_f_g_1) + \
              T.dot(h_l, self.W_f_g_2) + \
              self.b_f_g.dimshuffle('x', 0))

    return g_t

# for classification f_h uses simple rectify layer
# for dynamic environment f_h uses LSTM layer
def f_h(h_tm1, g_t): 
    """
    return: 
      h_t = hidden states (output of core network)
          = (n_batch x num hiddens of h_t) 
          [theano tensor variable] and recurrent

    inputs: 
      h_tm1 = hidden states estimated at t-1
            (n_batch x num hiddens of h_t) 
          [theano tensor variable] and recurrent 
      g_t = glimps output
          = (n_batch x num hiddens of g_t) 
          [theano tensor variable]

    parameters: 
      self.W_f_h_1 = (num hiddens of h_t x num hiddens of h_t)
      self.W_f_h_2 = (num hiddens of g_t x num hiddens of h_t)
      self.b_f_h = (num hiddens of h_t,)
    """
    h_t = lasagne.nonlinearities.rectify(\ 
              T.dot(h_tm1, self.W_f_h_1) + \ 
              T.dot(g_t, self.W_f_h_2) + \
              self.b_f_h.dimshuffle('x', 0))
    return h_t

def f_l(h_t): 
    """ 
    return: 
      loc_mean_t = (mean) location estimated for t
            = l(t) = y(t), x(t) 
            = (n_batch x 2) 
          [theano tensor variable] and recurrent

    inputs:
      h_t = hidden states (output of core network)
          = (n_batch x num hiddens of h_t)
          [theano tensor variable] and recurrent

    parameters: 
      self.W_f_l = (num hiddens of h_t x 2)
      self.b_f_l = (2,)   

    """
    loc_mean_t = T.dot(h_t, self.W_f_l) + self.b_f_l.dimshuffle('x', 0)
    return loc_mean_t

#def step(sequences, outputs, non_sequences, *varargin):
def step(loc_mean_tm1, loc_tm1, h_tm1, x, height, width, patch=8, k=1): 
    """
    return: 
    inputs: 
    parameters: 
      self.sigma = (2 x 2) whose diagonal is initialized with pre-defined standard deviations
    """
    x_t = rho(loc_t_tm1, loc_tm1, x, height, width, patch, k)
    g_t = f_g(x_t, loc_tm1)
    h_t = f_h(h_tm1, g_t)
    loc_mean_t = f_l(h_t)  

    #loc_t = gaussian(loc_mean_t, [[0.01, 0], [0, 0.01]])
    loc_t = loc_mean_t + T.dot(self._srng.normal(loc_mean_t.shape,
                                         avg=0.0,
                                         std=1.0), 
                       self.sigma)
    return loc_mean_t, loc_t, h_t

def classifier(h_T): 
    """
    return: 
      prob = (n_batch x num of classes)
          [theano tensor variable]
    inputs: 
    parameters: 
      self.W_classifier
      self.b_classifier
    """
    prob = lasagne.nonlinearities.softmax(\
               theano.T.dot(h_T, self.W_classifier) + \
               self.b_classifier.dimshuffle('x', 0))
    return prob

[loc_mean_t, loc_t, h_t], updates = theano.scan(step, 
                          outputs_info=[dict(initial=l_0, taps=None), # initial input of loc_mean_t does not affect the result
                                        dict(initial=l_0, taps=None), 
                                        dict(initial=h_0, taps=None)],
                          non_sequences=[x, height, width, patch, k], 
                          n_steps=n_steps)
prob = self.classifier(h_t[-1])
pred = T.argmax(prob, axis=1)

# loc_mean_t = (n_step x n_batch x 2)
# loc_t = (n_step x n_batch x 2)
# h_t = (n_step x n_batch x num hiddens of h_t) 
# prob = (n_batch x num classes)
# pred = (n_batch,) 

loc_mean_t = loc_mean_t.dimshuffle(1, 0, 2) # -> (n_batch x n_steps x 2)
loc_t = loc_t.dimshuffle(1, 0, 2)           # -> (n_batch x n_steps x 2)
h_t = h_t.dimshuffle(1, 0, 2)               # -> (n_batch x n_steps x num hiddens of h_t)

return loc_mean_t, loc_t, h_t, prod, pred

# member functions---------------------------------------------

def grad_reinforcement(self, labels)
    """
    return: 
      loss = 1 / M * sum_i_{1..M}{r_T^i}  where r_T is 1 (if correct) or 0 (if incorrect)
          [theano scalar variable]
      grad = 1 / M * sum_i_{1..M}{theano.grad( sum_t_{0..T}{logN(l_t | f_l(h_t))} ) * (R^i - b) )}
                 where R^i = r_T^i = 1 (if correct) or 0 (if incorrect)
                 b = mean(R^i)  (the value function???)
                 b = sum_i_{1..M}{( theano.grad( sum_t_{0..T}{logN(l_t | f_l(h_t))} ) ** 2 ) * R^i } / sum_i_{1..M}{ theano.grad( sum_t_{0..T}{logN(l_t | f_l(h_t))} ) ** 2 }
                 (optimal baseline)
          [theano tensor variable]

    inputs: 
      labels = (n_batch,) 
          [theano tensor variable]
    """
    loc_mean_t, loc_t, h_t, prod, pred = self.get_output_for()    
    params = self.get_params()
    n_batch = loc_t.shape[0]
    n_steps = loc_t.shape[1]

    ### baseline estimate
    tmp = - lasagne.objectives.squared_error(loc_t, loc_mean_t) / (2 * sigma**2)
    tmp = tmp.mean(axis=2) # -> (n_batch x n_steps)
    tmp = tmp.reshape((n_batch*n_steps,)) # -> (n_batch*n_steps, )

    jacobian = theano.gradient.jacobian(tmp, params) # -> (n_batch*n_steps, params.size())
    jacobian = jacobian.reshape((n_batch, n_steps, jacobian.shape[1])) # -> (n_batch, n_steps, params.size())

    r = theano.tensor.eq(pred, target_var) # -> (n_batch,)
    
    b = ((jacobian ** 2).mean(axis=1) * r.dimshuffle(0,'x') / (jacobian ** 2).mean(axis=1)).mean(axis=0)
    # = sum over n_batch_i { sum over n_steps_h {grad**2 * r_i} } / sum over n_batch_i { sum over n_step_h {grad**2} } 
    # -> (params.size(),)

    ### gradient estimate
    grad = (jacobian.mean(axis=1) * (r.dimshuffle(0, 'x') - b.dimshuffle('x', 0))).mean(axis=0)

    ### loss estimation
    loss = r.mean(axis=0)

    return loss, grad

def grad_supervised(self, labels)
    """
    return: 
      loss = 1 / M * sum_i_{1..M} cross_entroy_loss(groundtruth, a_T)
      grad = theano.grad(loss2)
    inputs: 
      labels = (n_batch,)
          [theano tensor variable]
    """
    loc_mean_t, loc_t, h_t, prod, pred = self.get_output_for()
    params = self.get_params()

    ### loss estimation (cross entropy loss)
    loss = categorical_crossentropy(probs, labels)
    loss = aggregate(loss, mode='mean')

    ### gradient estimation
    grad = theano.grad(loss, params)

    return loss, grad

def grad(self, labels, gamma=10.0): 
    """
    return: 
      loss 
      grad 
    """
    [loss1, grad1] = self.grad_reinforcement(labels)
    [loss2, grad2] = self.grad_supervised(labels)
    loss = loss1 + gamma * loss2
    grad = grad1 + gamma * grad2
    return loss, grad

#---------------------------------------------
